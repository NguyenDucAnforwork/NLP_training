{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwpIMQk02m5_",
        "outputId": "8dd7b3eb-8241-455f-d5b9-465b9ea38358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.50d.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exhzNDTq3wc_"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5WHVqrA323g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import re, nltk\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    cleaned_text = soup.get_text()\n",
        "    return cleaned_text\n",
        "\n",
        "html_text = \"\"\"[\"why doesn't an optical mouse work on a glass table? or even on some surfaces? optical mice use an led and a camera to rapidly capture images of the surface beneath the mouse.  the infomation from the camera is analyzed by a dsp (digital signal processor) and used to detect imperfections in the underlying surface and determine motion. some materials, such as glass, mirrors or other very shiny, uniform surfaces interfere with the ability of the dsp to accurately analyze the surface beneath the mouse.  \\\\nsince glass is transparent and very uniform, the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  mirrored surfaces are also a problem, since they constantly reflect back the same image, causing the dsp not to recognize motion properly. when the system is unable to see surface changes associated with movement, the mouse will not work properly. \", 'what is the best off-road motorcycle trail ? long-distance trail throughout ca i hear that the mojave road is amazing!<br />\\\\nsearch for it online. ', 'what is trans fat? how to reduce that? i heard that tras fat is bad for the body.  why is that? where can we find it in our daily food? trans fats occur in manufactured foods during the process of partial hydrogenation, when hydrogen gas is bubbled through vegetable oil to increase shelf life and stabilize the original polyunsatured oil. the resulting fat is similar to saturated fat, which raises \"bad\" ldl cholesterol and can lead to clogged arteries and heart disease. \\\\nuntil very recently, food labels were not required to list trans fats, and this health risk remained hidden to consumers. in early july, fda regulations changed, and food labels will soon begin identifying trans fat content in processed foods. ', 'how many planes fedex has? i heard that it is the largest airline in the world according to the www.fedex.com web site:\\\\nair fleet<br />\\\\n  <br />\\\\n670 aircraft, including: <br />\\\\n47 airbus a300-600s  17 boeing dc10-30s  <br />\\\\n62 airbus a310-200/300s  36 boeing md10-10s  <br />\\\\n2 atr 72s  5 boeing md10-30s  <br />\\\\n29 atr 42s  57 boeing md11s  <br />\\\\n18 boeing 727-100s  10 cessna 208as  <br />\\\\n94 boeing 727-200s  246 cessna 208bs  <br />\\\\n30 boeing dc10-10s  17 fokker f-27s ', 'in the san francisco bay area, does it make sense to rent or buy ? the prices of rent and the price of buying does not make sense to me, mostly the rent will not cover the mortgage . is it better to rent a house or to buy? renting vs buying depends on your goals. <br />\\\\ngenerally thinking is that buying is better b/c the payments that would go into the rent start building equity in your home. the govt also incentivizes you to buy by making your property tax payments and mortgage interest payments tax deductible.\\\\nhaving said that current housing status in the bay area is such that housing cost to purchase is relatively high and rental prices (compared to ownership cost) are relatively low (relative to the rest of the country). it makes lese sense to buy vs. other places.\\\\nbottom line you should base your decision on whether you think the market will keep going up or not. the other numbers tend to even out, the main gain or loss in buying comes from appreciation/depreciation. ']\"\"\"\n",
        "\n",
        "cleaned_text = remove_html_tags(html_text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "pcBJ5GAMeBVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, labels = [], []\n",
        "data_path = \"./train.csv\"\n",
        "with open(data_path) as csv_file:\n",
        "    reader = csv.reader(csv_file, quotechar='\"')\n",
        "    for idx, line in enumerate(reader):\n",
        "        text = \"\"\n",
        "        for tx in line[1:]:\n",
        "            feature_cleaned = tx.replace('\"', '').replace('\\\\n', ' ').replace('\\\\', ' ').replace(',', ' ')\n",
        "            text += feature_cleaned.lower()\n",
        "            text += \" \"\n",
        "        label = int(line[0]) - 1\n",
        "        texts.append(text)\n",
        "        labels.append(label)"
      ],
      "metadata": {
        "id": "oe_ik48Hc6b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO5jiHkAgAvW",
        "outputId": "f1193a40-bbd4-4f83-da61-112f819f4b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is the best off-road motorcycle trail ? long-distance trail throughout ca i hear that the mojave road is amazing!<br /> search for it online. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    cleaned_text = soup.get_text()\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = re.sub(r'[?.!]+$', '', sentence)\n",
        "        cleaned_sentences.append(cleaned_sentence.strip())\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "Zog8nDKHgg9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blL3i5QH4PPk"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    nltk.download('punkt')\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    words = [word for word in words if re.match(r'^[a-zA-Z]+$', word)]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e322tmab4AfS"
      },
      "outputs": [],
      "source": [
        "def WordEmbedding(sentence):\n",
        "    words = preprocess_sentence(sentence)\n",
        "    sentence_embedding = []\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "          word_embedding = torch.tensor(vocab[word]).float()  # Load embedding from Glove model\n",
        "        else:\n",
        "          word_embedding = torch.randn(50)\n",
        "        sentence_embedding.append(word_embedding)\n",
        "\n",
        "    if sentence_embedding:\n",
        "        return torch.stack(sentence_embedding)  # Convert list of tensors to a single tensor\n",
        "    else:\n",
        "        return torch.randn(1, 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_cleaned = [split_into_sentences(text) for text in texts]"
      ],
      "metadata": {
        "id": "UzGRurs2ehCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts_cleaned[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTV5ZcTufYVC",
        "outputId": "fb437996-2580-47f1-f7b7-ae22eb3a16b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what is the best off-road motorcycle trail', 'long-distance trail throughout ca i hear that the mojave road is amazing', 'search for it online']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = texts_cleaned[5]"
      ],
      "metadata": {
        "id": "EoDsIg1NKVMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sent.strip() for sent in doc]\n",
        "sentences = [WordEmbedding(sentence) for sentence in sentences]\n",
        "doc_length = len(sentences)\n",
        "sent_length = max([len(sent) for sent in sentences])\n"
      ],
      "metadata": {
        "id": "955AoTFuWlYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for line in lines:\n",
        "    part = line.strip().split(\"\\t\")\n",
        "    sample = part[0].strip('][').split(',')\n",
        "    label = sample[0].strip('\"')\n",
        "    feature = ','.join(sample[1:]).strip('\"')\n",
        "    feature_cleaned = feature.replace('\"', '').replace('\\\\n', ' ').replace('\\\\', '').replace(',', ' ').replace('?', ' ')\n",
        "    data.append([label, feature_cleaned])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Label\", \"Feature\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "BouTgY7sMxv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Feature'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-6kN8X3a6tj",
        "outputId": "89e6f2af-7475-4968-a2d6-252282f5ad47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why doesn't an optical mouse work on a glass table  or even on some surfaces  Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect imperfections in the underlying surface and determine motion. Some materials  such as glass  mirrors or other very shiny  uniform surfaces interfere with the ability of the DSP to accurately analyze the surface beneath the mouse.   Since glass is transparent and very uniform  the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  Mirrored surfaces are also a problem  since they constantly reflect back the same image  causing the DSP not to recognize motion properly. When the system is unable to see surface changes associated with movement  the mouse will not work properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './glove.6B.50d.txt'\n",
        "\n",
        "vocab = {}\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split()\n",
        "        word = parts[0]\n",
        "\n",
        "        vector = torch.tensor([float(val) for val in parts[1:]])\n",
        "        vocab[word] = vector"
      ],
      "metadata": {
        "id": "Af9TmnOs3h7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(han.parameters(), lr=0.001)\n",
        "\n",
        "# number of epochs\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for doc, label in zip(texts_cleaned, labels):\n",
        "        sentences = [sent.strip() for sent in doc]\n",
        "        sentences = [WordEmbedding(sentence) for sentence in sentences]\n",
        "        doc_length = len(sentences)\n",
        "        sent_length = max([len(sent) for sent in sentences])\n",
        "        han = HAN(sent_length, doc_length, 50, 20, 40, 10)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = han(doc)\n",
        "\n",
        "        label_index = int(label)\n",
        "        num_classes = 10  # Số lượng lớp (nhãn)\n",
        "        label_tensor = torch.zeros(num_classes)\n",
        "        label_tensor[label_index-1] = 1\n",
        "        loss = criterion(outputs, label_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n"
      ],
      "metadata": {
        "id": "Sr0C0WkYSoGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab['i'])"
      ],
      "metadata": {
        "id": "fC7LsUwh3tmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"The final week of the NBA regular season has plenty of play-in tournament drama on. \",\n",
        "             \"None of the four play-in spots in the Eastern and Western Conferences are set going into games\",\n",
        "             \"The Eastern Conference race is far more since the Indiana, opponents left on their respective schedule\"]"
      ],
      "metadata": {
        "id": "JRnUqFpK0dp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [(WordEmbedding(sentence)) for sentence in sentences]\n",
        "sentences = pad_sequence([torch.tensor(sentence) for sentence in sentences], batch_first=True)   # padding\n"
      ],
      "metadata": {
        "id": "AJfOEW3q1QDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((sentences).shape)"
      ],
      "metadata": {
        "id": "WnwWtmEV3Sk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04at8KWwAIAG"
      },
      "outputs": [],
      "source": [
        "class WordEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_units):\n",
        "        super(WordEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_units, bidirectional=True)\n",
        "        self.hidden_state = torch.zeros(2, hidden_units)\n",
        "    def forward(self, sentence_embedding):   # hidden state = [2, batch_size, hidden_size] cuz it's bidirectional\n",
        "        outputs, _ = self.gru(sentence_embedding, self.hidden_state)\n",
        "        return outputs   # [sentence_length, 2 * hidden_units]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9meGmqPcE8RO"
      },
      "outputs": [],
      "source": [
        "class WordAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "      super(WordAttention, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.score_weight_matrix = nn.Linear(input_size, hidden_size, bias=True)\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "      self.context_vector = nn.Parameter(torch.randn(1, hidden_size))\n",
        "\n",
        "    def forward(self, output_tensor):\n",
        "      u_i = torch.tanh(self.score_weight_matrix(output_tensor))   # [batch_size, T, hidden_size]\n",
        "      dot_product = torch.matmul(u_i, self.context_vector.transpose(0, 1))  # [batch_size, T, hidden_size]\n",
        "      a_i = nn.Softmax(dim=0)(dot_product)  # [batch_size, T, hidden_size]\n",
        "      s_i = (a_i * output_tensor).sum(dim=1)\n",
        "      return s_i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_units):\n",
        "        super(SentenceEncoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_units, bidirectional=True)   # [sentence_length * (2 * hidden_units)]\n",
        "        self.hidden_state = torch.zeros(2, hidden_units)\n",
        "\n",
        "    def forward(self, document_embedding):\n",
        "        outputs, _ = self.gru(document_embedding, self.hidden_state)\n",
        "        return outputs   # [sentence_length, 2 * hidden_units]"
      ],
      "metadata": {
        "id": "B7vYTbWy-r9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "      super(SentenceAttention, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.score_weight_matrix = nn.Linear(input_size, hidden_size, bias=True)\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "      self.context_vector = nn.Parameter(torch.randn(1, hidden_size))\n",
        "\n",
        "    def forward(self, encoded_sent):\n",
        "      u_i = torch.tanh(self.score_weight_matrix(encoded_sent))   # [batch_size, T, hidden_size]\n",
        "      dot_product = torch.matmul(u_i, self.context_vector.transpose(0, 1))  # [batch_size, T, hidden_size]\n",
        "      a_i = nn.Softmax(dim=0)(dot_product)  # [batch_size, T, hidden_size]\n",
        "      s_i = (a_i * encoded_sent).sum(dim=1)\n",
        "      return s_i"
      ],
      "metadata": {
        "id": "SwC4zUe-IiCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentClassification(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(DocumentClassification, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.linear = nn.Linear(input_size, output_size, bias=True)\n",
        "  def forward(self, encoded_doc):\n",
        "    p = self.linear(encoded_doc)\n",
        "    p = nn.Softmax(dim=0)(p)\n",
        "    return p\n"
      ],
      "metadata": {
        "id": "nzVyfaWeS6qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HAN(nn.Module):\n",
        "  def __init__(self, sent_length, doc_length, embedding_dim, hidden_units, context_size, output_size):\n",
        "    super(HAN, self).__init__()\n",
        "    self.word_encoder = WordEncoder(embedding_dim, hidden_units)\n",
        "    self.word_att = WordAttention(2 * hidden_units, context_size)\n",
        "    self.sentence_encoder = SentenceEncoder(sent_length, hidden_units)\n",
        "    self.sentence_att = SentenceAttention(2 * hidden_units, doc_length)\n",
        "    self.doc_classify = DocumentClassification(doc_length, output_size)\n",
        "\n",
        "  def forward(self, doc):\n",
        "    sentences = [sent.strip() for sent in doc]\n",
        "    sentences = [WordEmbedding(sentence) for sentence in sentences]\n",
        "    sentences = pad_sequence(([torch.tensor(sentence) for sentence in sentences]), batch_first=True, padding_value=0)\n",
        "    document_embedding = []\n",
        "    for sentence_embedding in sentences:\n",
        "      encoded_sent = self.word_encoder(sentence_embedding)\n",
        "      att_sent = self.word_att(encoded_sent)\n",
        "      document_embedding.append(att_sent)\n",
        "    document_embedding = torch.stack(document_embedding) # if it needs to be 3D dimensional then add .unsqueeze(1)\n",
        "    sentence_encoder = self.sentence_encoder(document_embedding)   # 16: max number of word in a sentence\n",
        "    encoded_doc = self.sentence_att(sentence_encoder)\n",
        "    predictions = self.doc_classify(encoded_doc)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "ubl8u4dke-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "han = HAN(sent_length, doc_length, 50, 20, 40, 10)\n",
        "han(doc)"
      ],
      "metadata": {
        "id": "_92V-zpxX_eT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}